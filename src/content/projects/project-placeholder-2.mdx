---
title: "ML Feature Store Platform"
description: "A centralized feature engineering platform enabling ML teams to discover, share, and serve features at scale."
publishedAt: 2024-01-20
updatedAt: 2024-08-05
tags: ["Python", "FastAPI", "Apache Spark", "Redis", "MLflow", "Kubernetes"]
projectType: "case-study"
status: "completed"
role: "ML Infrastructure Engineer"
repoUrl: "https://github.com/{{GITHUB_USERNAME}}/feature-store"
highlights:
  - "Reduced feature engineering time by 70%"
  - "Serves 500K feature requests per second"
  - "Used by 15+ ML models in production"
---

## Problem

ML teams were spending 80% of their time on feature engineering, with significant duplication of effort across teams. Features weren't reusable, training-serving skew was common, and there was no visibility into feature lineage or quality.

## Context

The company had multiple ML teams building models for different use casesâ€”fraud detection, recommendations, pricing optimization. Each team maintained their own feature pipelines, leading to inconsistencies and wasted effort.

## Role & Scope

As ML Infrastructure Engineer, I owned:
- Feature store architecture and API design
- Online serving infrastructure (low-latency feature retrieval)
- Integration with existing ML pipelines
- Developer experience and documentation

## Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Feature Registry | PostgreSQL + FastAPI | Metadata management |
| Offline Store | Apache Spark + Delta Lake | Batch feature computation |
| Online Store | Redis Cluster | Low-latency serving |
| Orchestration | Airflow | Pipeline scheduling |
| Experiment Tracking | MLflow | Feature experiment management |

## Architecture

The platform follows a medallion architecture:

1. **Bronze Layer**: Raw data ingestion from various sources
2. **Silver Layer**: Cleaned and normalized entities
3. **Gold Layer**: Feature tables ready for consumption
4. **Serving Layer**: Materialized features in Redis for online inference

Key abstractions:
- **Feature View**: Logical grouping of features for an entity
- **Feature Service**: Production-ready feature set for a model
- **Data Source**: Configurable data ingestion connector

## Key Decisions & Trade-offs

### Delta Lake for Offline Storage

**Decision**: Used Delta Lake instead of raw Parquet files.

**Trade-off**: Added complexity but gained ACID transactions, time travel, and schema evolution.

**Outcome**: Enabled point-in-time correct training data generation, eliminating training-serving skew.

### Redis over Feature-Specific Databases

**Decision**: Used Redis Cluster for all online serving instead of specialized databases.

**Trade-off**: Redis isn't optimized for every access pattern, but operational simplicity won.

**Outcome**: Single system to operate, monitor, and debug. p99 latency under 5ms.

## Challenges

### Challenge 1: Point-in-Time Correctness

**Problem**: Training data was leaking future information, causing models to perform worse in production than in backtests.

**Solution**: Implemented event-time processing with watermarks and temporal joins. All feature retrievals require an event timestamp.

### Challenge 2: Feature Discovery

**Problem**: Teams didn't know what features existed, leading to duplicate development.

**Solution**: Built a feature catalog with rich metadata, lineage graphs, usage statistics, and quality scores.

## Results

| Metric | Impact |
|--------|--------|
| Feature engineering time | 70% reduction |
| Feature reuse rate | 60% of features used by 2+ teams |
| Training-serving skew incidents | 0 (down from 3/month) |
| Online serving latency (p99) | 5ms |
| Feature request throughput | 500K/sec |

## What I'd Improve

1. **Self-service onboarding**: Currently requires infrastructure team involvement. A CLI wizard would enable teams to onboard independently.

2. **Feature monitoring**: We track operational metrics but not feature distribution drift. Adding automated drift detection would catch data quality issues earlier.

3. **GPU-accelerated transformations**: Some feature transformations are CPU-bound. GPU acceleration would reduce batch processing time significantly.

## Links

- [GitHub Repository](https://github.com/{{GITHUB_USERNAME}}/feature-store)
- [Documentation](https://feature-store.{{YOUR_DOMAIN}}/docs)

